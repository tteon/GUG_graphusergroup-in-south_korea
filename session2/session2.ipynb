{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792e5e8c",
   "metadata": {},
   "source": [
    "This session motivated by [post](https://medium.com/stanford-cs224w/buy-this-session-based-recommendation-using-sr-gnn-d3415e393722) and how capture the each users' intention in the e-commerce system. so i planned this tutorial. i hope you guys this tutorial help to your research or application idea. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb567f",
   "metadata": {},
   "source": [
    " # library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bdc5465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/graph/lib/python3.9/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022\n",
      "  warnings.warn(\"pyprof will be removed by the end of June, 2022\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Python built-in libraries\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Import torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "# Import PyG packages\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.data as pyg_data\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "import torch_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20423ad",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "pd.factorize()\n",
    "torch.bincount()\n",
    "np.argpartition()\n",
    "\n",
    "Error\n",
    "\n",
    "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling cublasCreate(handle)\n",
    "--> torch.nn.Embedding(embed , hidden_size) resize error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c25fa",
   "metadata": {},
   "source": [
    "## pyg dataset tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29532f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e582b0d9",
   "metadata": {},
   "source": [
    "## pyg message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453e468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc966964",
   "metadata": {},
   "source": [
    "# environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fea9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you hope to get this dataset , visit this link http://www.recsyschallenge.com/2022/\n",
    "os.chdir('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d19071",
   "metadata": {},
   "source": [
    "# dataset overview\n",
    "\n",
    "[link](http://www.recsyschallenge.com/2022/)\n",
    "\n",
    "\n",
    "- Sessions: The items that were viewed in a session. In this dataset a session is equal to a day, so a session is one user's activity on one day.\n",
    "\n",
    "\n",
    "- Purchases: The purchase that happened at the end of the session. One purchased item per session.\n",
    "\n",
    "\n",
    "- Item features: The label data of items. Things like “color: green,” “neckline: v-neck,” etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400cbe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## original resource is converted the timestamp to unixtime because the session building.\n",
    "## but, our dataset already has the session setup. we didn't that process\n",
    "# timestamp to unixtime\n",
    "candidate_item = pd.read_csv('candidate_items.csv')\n",
    "item_df = pd.read_csv('item_features.csv')\n",
    "train_df = pd.read_csv('train_sessions.csv')\n",
    "train_label = pd.read_csv('train_purchases.csv')\n",
    "# train['timestamp'] = pd.to_datetime(train['date'])\n",
    "# train['unixtime'] = train['timestamp'].map(lambda x : time.mktime(x.timetuple()))\n",
    "# train_label['timestamp'] = pd.to_datetime(train_label['date'])\n",
    "# train_label['unixtime'] = train_label['timestamp'].map(lambda x : time.mktime(x.timetuple()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036aaa9",
   "metadata": {},
   "source": [
    "# dataset explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ae1376c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)\n",
    "\n",
    "CounterA = Counter(item[item['item_id'] == 25357].sort_values('feature_category_id')['feature_value_id'])\n",
    "CounterB = Counter(item[item['item_id'] == 21630].sort_values('feature_category_id')['feature_value_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fcd3672e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64970"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(item['feature_category_id'].nunique())\n",
    "print(item['feature_value_id'].nunique())\n",
    "73 * 890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "582eb9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>feature_category_id</th>\n",
       "      <th>feature_value_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>425904</th>\n",
       "      <td>25357</td>\n",
       "      <td>3</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425907</th>\n",
       "      <td>25357</td>\n",
       "      <td>4</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425901</th>\n",
       "      <td>25357</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425908</th>\n",
       "      <td>25357</td>\n",
       "      <td>7</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425922</th>\n",
       "      <td>25357</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425917</th>\n",
       "      <td>25357</td>\n",
       "      <td>17</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425919</th>\n",
       "      <td>25357</td>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425902</th>\n",
       "      <td>25357</td>\n",
       "      <td>26</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425921</th>\n",
       "      <td>25357</td>\n",
       "      <td>32</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425918</th>\n",
       "      <td>25357</td>\n",
       "      <td>34</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425905</th>\n",
       "      <td>25357</td>\n",
       "      <td>41</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425911</th>\n",
       "      <td>25357</td>\n",
       "      <td>44</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425920</th>\n",
       "      <td>25357</td>\n",
       "      <td>45</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425906</th>\n",
       "      <td>25357</td>\n",
       "      <td>46</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425913</th>\n",
       "      <td>25357</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425923</th>\n",
       "      <td>25357</td>\n",
       "      <td>50</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425914</th>\n",
       "      <td>25357</td>\n",
       "      <td>55</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425899</th>\n",
       "      <td>25357</td>\n",
       "      <td>56</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425912</th>\n",
       "      <td>25357</td>\n",
       "      <td>61</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425915</th>\n",
       "      <td>25357</td>\n",
       "      <td>63</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425903</th>\n",
       "      <td>25357</td>\n",
       "      <td>65</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425916</th>\n",
       "      <td>25357</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425900</th>\n",
       "      <td>25357</td>\n",
       "      <td>69</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425910</th>\n",
       "      <td>25357</td>\n",
       "      <td>72</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425909</th>\n",
       "      <td>25357</td>\n",
       "      <td>73</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id  feature_category_id  feature_value_id\n",
       "425904    25357                    3               793\n",
       "425907    25357                    4               618\n",
       "425901    25357                    5                58\n",
       "425908    25357                    7               452\n",
       "425922    25357                   15               133\n",
       "425917    25357                   17               378\n",
       "425919    25357                   19               112\n",
       "425902    25357                   26               126\n",
       "425921    25357                   32               352\n",
       "425918    25357                   34               275\n",
       "425905    25357                   41               181\n",
       "425911    25357                   44               615\n",
       "425920    25357                   45               559\n",
       "425906    25357                   46               825\n",
       "425913    25357                   47                36\n",
       "425923    25357                   50               240\n",
       "425914    25357                   55               129\n",
       "425899    25357                   56               365\n",
       "425912    25357                   61               706\n",
       "425915    25357                   63               861\n",
       "425903    25357                   65               521\n",
       "425916    25357                   68                31\n",
       "425900    25357                   69               592\n",
       "425910    25357                   72               263\n",
       "425909    25357                   73               544"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[item['item_id'] == 25357].sort_values('feature_category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "510e2b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>feature_category_id</th>\n",
       "      <th>feature_value_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363330</th>\n",
       "      <td>21630</td>\n",
       "      <td>3</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363313</th>\n",
       "      <td>21630</td>\n",
       "      <td>4</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363316</th>\n",
       "      <td>21630</td>\n",
       "      <td>5</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363328</th>\n",
       "      <td>21630</td>\n",
       "      <td>7</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363321</th>\n",
       "      <td>21630</td>\n",
       "      <td>15</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363327</th>\n",
       "      <td>21630</td>\n",
       "      <td>17</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363325</th>\n",
       "      <td>21630</td>\n",
       "      <td>19</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363329</th>\n",
       "      <td>21630</td>\n",
       "      <td>26</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363308</th>\n",
       "      <td>21630</td>\n",
       "      <td>32</td>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363311</th>\n",
       "      <td>21630</td>\n",
       "      <td>41</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363320</th>\n",
       "      <td>21630</td>\n",
       "      <td>44</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363309</th>\n",
       "      <td>21630</td>\n",
       "      <td>45</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363314</th>\n",
       "      <td>21630</td>\n",
       "      <td>46</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363317</th>\n",
       "      <td>21630</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363315</th>\n",
       "      <td>21630</td>\n",
       "      <td>50</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363331</th>\n",
       "      <td>21630</td>\n",
       "      <td>55</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363319</th>\n",
       "      <td>21630</td>\n",
       "      <td>56</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363310</th>\n",
       "      <td>21630</td>\n",
       "      <td>61</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363322</th>\n",
       "      <td>21630</td>\n",
       "      <td>63</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363324</th>\n",
       "      <td>21630</td>\n",
       "      <td>65</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363326</th>\n",
       "      <td>21630</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363312</th>\n",
       "      <td>21630</td>\n",
       "      <td>69</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363323</th>\n",
       "      <td>21630</td>\n",
       "      <td>72</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363318</th>\n",
       "      <td>21630</td>\n",
       "      <td>73</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id  feature_category_id  feature_value_id\n",
       "363330    21630                    3               793\n",
       "363313    21630                    4               618\n",
       "363316    21630                    5               605\n",
       "363328    21630                    7               452\n",
       "363321    21630                   15               133\n",
       "363327    21630                   17               378\n",
       "363325    21630                   19               765\n",
       "363329    21630                   26               126\n",
       "363308    21630                   32               902\n",
       "363311    21630                   41               145\n",
       "363320    21630                   44               509\n",
       "363309    21630                   45               559\n",
       "363314    21630                   46               825\n",
       "363317    21630                   47                36\n",
       "363315    21630                   50               240\n",
       "363331    21630                   55               129\n",
       "363319    21630                   56               365\n",
       "363310    21630                   61               706\n",
       "363322    21630                   63               861\n",
       "363324    21630                   65               521\n",
       "363326    21630                   68                31\n",
       "363312    21630                   69               592\n",
       "363323    21630                   72                75\n",
       "363318    21630                   73               544"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[item['item_id'] == 21630].sort_values('feature_category_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e5dae",
   "metadata": {},
   "source": [
    "# data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95a1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view action\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4464891</th>\n",
       "      <td>4179694</td>\n",
       "      <td>12969</td>\n",
       "      <td>2020-07-10 06:53:37.397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         session_id  item_id                     date\n",
       "4464891     4179694    12969  2020-07-10 06:53:37.397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy action\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941120</th>\n",
       "      <td>4179694</td>\n",
       "      <td>11060</td>\n",
       "      <td>2020-07-10 06:53:59.982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id  item_id                     date\n",
       "941120     4179694    11060  2020-07-10 06:53:59.982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sessionlist = list(set(train_df['session_id'].values))\n",
    "\n",
    "session_ = np.random.choice(sessionlist, size=1)\n",
    "print('view action')\n",
    "display(train_df[train_df['session_id'] == session_[0]])\n",
    "print('buy action')\n",
    "display(train_label[train_label['session_id'] == session_[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a18f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view action\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2313, 19499,  4741, 16985, 14330, 21317])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy action\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5599])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the session action list is [2313, 19499, 4741, 16985, 14330, 21317, 5599]\n"
     ]
    }
   ],
   "source": [
    "session_ = np.random.choice(sessionlist, size=1)\n",
    "\n",
    "print('view action')\n",
    "display(train_df[train_df['session_id'] == session_[0]]['item_id'].values)\n",
    "view_action = train_df[train_df['session_id'] == session_[0]]['item_id'].values\n",
    "print('buy action')\n",
    "display(train_label[train_label['session_id'] == session_[0]]['item_id'].values)\n",
    "buy_action = train_label[train_label['session_id'] == session_[0]]['item_id'].values\n",
    "\n",
    "session_list = np.concatenate((view_action, buy_action)).tolist()\n",
    "print(f'the session action list is {session_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81065ff1",
   "metadata": {},
   "source": [
    "# session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae269bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_session=[]\n",
    "    for sessions_ in tqdm(sessionlist):\n",
    "        viewaction = train_df[train_df['session_id'] == sessions_]['item_id'].values\n",
    "        #print(viewaction)\n",
    "        buyaction = train_label[train_label['session_id'] == sessions_]['item_id'].values\n",
    "        #print(buyaction)\n",
    "        all_session.append(np.concatenate((viewaction, buyaction)).tolist())\n",
    "        #print(all_session)\n",
    "    np.save('all_session',all_session)\n",
    "except:\n",
    "    all_session = np.load('all_session.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebad14fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13596, 13596, 14392]\n",
      "[9655, 9655, 15085]\n",
      "[2814, 10096, 10096, 10373, 10096, 12251]\n",
      "[5407, 15039, 6961, 17919]\n",
      "[5154, 23282]\n",
      "[24067, 4139, 9184]\n",
      "[27173, 26160, 20412, 4179, 20412, 1107, 20412, 127, 22926, 22926, 25146, 18061, 24801]\n",
      "[27349, 20495]\n",
      "[24362, 11053, 26691, 11382]\n",
      "[15654, 18626]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):print(all_session[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fa396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the sequence of the event [13596, 13596],and the action(buy) [14392]\n"
     ]
    }
   ],
   "source": [
    "#example session format\n",
    "\n",
    "print(f'train the sequence of the event {all_session[0][:-1]},and the action(buy) {[all_session[0][-1]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b7e56d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our datasets didn't have the visitors so, skipping this secton.\n",
    "\n",
    "# def extract_subsessions(sessions):\n",
    "#     '''Extracts all partial sessions from the sessions given.\n",
    "    \n",
    "#     (1,2,3) -> (1,2) and (1,2,3)\n",
    "    \n",
    "#     '''\n",
    "#     all_sessions = []\n",
    "#     for session in sessions:\n",
    "#         for i in range(1, len(session)):\n",
    "#             all_sessions.append(session[:i+1])\n",
    "#     return all_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34e2ea",
   "metadata": {},
   "source": [
    "# train test val splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a059b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 1\n",
    "seed = 42\n",
    "\n",
    "num_train = int(len(all_session) * 0.8 * sampling_rate)\n",
    "num_val = int(len(all_session) * 0.1 * sampling_rate)\n",
    "num_test = int(len(all_session) * 0.1 * sampling_rate)\n",
    "\n",
    "train_sessions = all_session[:num_train]\n",
    "val_sessions = all_session[num_train : num_train+num_val]\n",
    "test_sessions = all_session[num_train+num_val : num_train+num_val+num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3387eaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, and test sessions: (800000, 100000, 100000)\n",
      "the raw made sucessfully.\n"
     ]
    }
   ],
   "source": [
    "# Check the number of (sub)sessions in each split\n",
    "print(f'train, val, and test sessions: {len(train_sessions), len(val_sessions), len(test_sessions)}')\n",
    "\n",
    "directory = 'raw'\n",
    "if os.path.isdir(directory)==False:\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "print(f'the {directory} made sucessfully.')\n",
    "    \n",
    "# Save the processed files.\n",
    "with open('raw/train.txt', 'wb') as f:\n",
    "    pickle.dump(train_sessions, f)\n",
    "with open('raw/val.txt', 'wb') as f:\n",
    "    pickle.dump(val_sessions, f)\n",
    "with open('raw/test.txt', 'wb') as f:\n",
    "    pickle.dump(test_sessions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3dc2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load for sessions\n",
    "path = 'raw/train.txt'\n",
    "with open(path, 'rb') as f:\n",
    "    pkl_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035718e7",
   "metadata": {},
   "source": [
    "## dataset formatting for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcef9447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the session seqeunce(view action) is [16462] \n",
      "\n",
      "\n",
      "the session result(buy action) is [12955] \n",
      "\n",
      "\n",
      "[0] \n",
      "\n",
      "[16462]\n"
     ]
    }
   ],
   "source": [
    "session_num = 123345\n",
    "session, y = all_session[session_num][:-1], all_session[session_num][-1]\n",
    "print(f'the session seqeunce(view action) is {session} \\n\\n')\n",
    "print(f'the session result(buy action) is {[y]} \\n\\n')\n",
    "\n",
    "codes, uniques = pd.factorize(session)\n",
    "\n",
    "print(codes , '\\n')\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a94585d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(codes) , len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "senders, receivers = codes[:-1], codes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "27775bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9  1 10 11] \n",
      "\n",
      "[ 1  2  3  4  5  6  7  8  9  1 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(senders, '\\n')\n",
    "print(receivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c928e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(pyg_data.InMemoryDataset):\n",
    "    def __init__(self, root, file_name, transform=None, pre_transform=None):\n",
    "        self.file_name = file_name\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f'{self.file_name}.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'{self.file_name}.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        raw_data_file = f'{self.raw_dir}/{self.raw_file_names[0]}'\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            sessions = pickle.load(f)\n",
    "        data_list = []\n",
    "\n",
    "        for session in sessions:\n",
    "            session, y = session[:-1], session[-1]\n",
    "            codes, uniques = pd.factorize(session)\n",
    "            senders, receivers = codes[:-1], codes[1:]\n",
    "\n",
    "            # Build Data instance\n",
    "            edge_index = torch.tensor([senders, receivers], dtype=torch.long)\n",
    "            x = torch.tensor(uniques, dtype=torch.long).unsqueeze(1)\n",
    "            y = torch.tensor([y], dtype=torch.long)\n",
    "            data_list.append(pyg_data.Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cceba7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSessionGraphConv(pyg.nn.conv.MessagePassing):\n",
    "    # you can assign the aggregation function ex) sum , mean , add etc\n",
    "    def __init__(self, out_channels, aggr: str ='add', **kwargs):\n",
    "        super().__init__(aggr=aggr, **kwargs)\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.gru = torch.nn.GRUCell(out_channels, out_channels, bias=False)\n",
    "    \n",
    "    # edge index X node feature\n",
    "    ## In this section , Done both aggregate and propagate work for neighbour information  fetching.\n",
    "    def forward(self, x, edge_index):\n",
    "        m = self.propagate(edge_index, x=x, size=None)\n",
    "        x = self.gru(m , x)\n",
    "        return x\n",
    "    \n",
    "    # source to target\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "    \n",
    "    # propagate and update\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bc0bd",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e50a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters.\n",
    "# Code taken from 2021 Fall CS224W Colab assignments.\n",
    "# num_items = max(item.item_id)\n",
    "# num_items = train_label.item_id.nunique()\n",
    "# num_items = len(set(candidate_item.item_id))\n",
    "# num_items = item_df.item_id.nunique()\n",
    "args = {\n",
    "    'batch_size': 100,\n",
    "    'hidden_dim': 32,\n",
    "    'epochs': 100,\n",
    "    'l2_penalty': 0.00001,\n",
    "    'weight_decay': 0.1,\n",
    "    'step': 30,\n",
    "    'lr': 0.001,\n",
    "    'num_items': 30000}\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d): \n",
    "        self.__dict__ = d\n",
    "\n",
    "args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15dc8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Prepare data pipeline\n",
    "    train_dataset = GraphDataset('./', 'train')\n",
    "    train_loader = pyg_data.DataLoader(train_dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=True)\n",
    "    val_dataset = GraphDataset('./', 'val')\n",
    "    val_loader = pyg_data.DataLoader(val_dataset,\n",
    "                                     batch_size=args.batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     drop_last=True)\n",
    "\n",
    "    # Build model\n",
    "    model = SRGNN(args.hidden_dim, args.num_items).to('cuda')\n",
    "\n",
    "    # Get training components\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.l2_penalty)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step,\n",
    "                                          gamma=args.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    top_k_accs = []\n",
    "\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for _, batch in enumerate(tqdm(train_loader)):\n",
    "            batch.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            loss = criterion(pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "          test_acc, top_k_acc = test(val_loader, model, is_validation=True)\n",
    "          print(test_acc)\n",
    "          test_accs.append(test_acc)\n",
    "          top_k_accs.append(top_k_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "  \n",
    "    return test_accs, top_k_accs, losses, best_model, best_acc, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b14a29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, test_model, is_validation=False, save_model_preds=False):\n",
    "    test_model.eval()\n",
    "\n",
    "    # Define K for Hit@K metrics.\n",
    "    k = 20\n",
    "    correct = 0\n",
    "    top_k_correct = 0\n",
    "\n",
    "    for _, data in enumerate(tqdm(loader)):\n",
    "        data.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            score = test_model(data)\n",
    "            pred = score.max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        if save_model_preds:\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('pred.csv', sep=',', index=False)\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "        # We calculate Hit@K accuracy only at test time.\n",
    "        if not is_validation:\n",
    "            score = score.cpu().detach().numpy()\n",
    "            for row in range(pred.size(0)):\n",
    "                top_k_pred = np.argpartition(score[row], -k)[-k:]\n",
    "                if label[row].item() in top_k_pred:\n",
    "                    top_k_correct += 1\n",
    "    \n",
    "    if not is_validation:\n",
    "        return correct / len(loader), top_k_correct / len(loader)\n",
    "    else:\n",
    "        return correct / len(loader), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "998fdce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGNN(nn.Module):\n",
    "    def __init__(self, hidden_size, n_items):\n",
    "        super(SRGNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_items = n_items\n",
    "        \n",
    "        # node features \n",
    "        self.embedding = nn.Embedding(self.n_items, self.hidden_size)\n",
    "        self.gated = GatedSessionGraphConv(self.hidden_size)\n",
    "\n",
    "        self.q = nn.Linear(self.hidden_size, 1)\n",
    "        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_map = data.x, data.edge_index, data.batch\n",
    " \n",
    "        #### TO DO LIST\n",
    "        # you could feature update in this section.\n",
    "        # not embed weight value (lookup) , using your own feature of node.\n",
    "        # ex) the value of item , and derivation of feature.\n",
    "        \n",
    "        # (0)\n",
    "        embedding = self.embedding(x).squeeze()\n",
    "\n",
    "        # (1)-(5)\n",
    "        v_i = self.gated(embedding, edge_index)\n",
    "\n",
    "        # Divide nodes by session\n",
    "        # For the detailed explanation of what is happening below, please refer\n",
    "        # to the Medium blog post.\n",
    "        sections = list(torch.bincount(batch_map).cpu())\n",
    "        v_i_split = torch.split(v_i, sections)\n",
    "\n",
    "        v_n, v_n_repeat = [], []\n",
    "        for session in v_i_split:\n",
    "            v_n.append(session[-1])\n",
    "            v_n_repeat.append(\n",
    "                session[-1].view(1, -1).repeat(session.shape[0], 1))\n",
    "        v_n, v_n_repeat = torch.stack(v_n), torch.cat(v_n_repeat, dim=0)\n",
    "\n",
    "        q1 = self.W_1(v_n_repeat)\n",
    "        q2 = self.W_2(v_i)\n",
    "\n",
    "        # (6)\n",
    "        alpha = self.q(F.sigmoid(q1 + q2))\n",
    "        s_g_split = torch.split(alpha * v_i, sections)\n",
    "\n",
    "        s_g = []\n",
    "        for session in s_g_split:\n",
    "            s_g_session = torch.sum(session, dim=0)\n",
    "            s_g.append(s_g_session)\n",
    "        s_g = torch.stack(s_g)\n",
    "\n",
    "        # (7)\n",
    "        # global embedding\n",
    "        ## the weighted average of the embeddigns of the items in the session.\n",
    "        ### such as 'ATTENTION'\n",
    "        s_l = v_n\n",
    "        s_h = self.W_3(torch.cat([s_l, s_g], dim=-1))\n",
    "\n",
    "        # (8) Unnormalized scroes \n",
    "        ## computing the cosine similairty between the session embedding (1 x d)\n",
    "        ## if the embeddings of all 499867 items, then the un-normalized score will \n",
    "        ## (466867 x 1). This represents the scores for each item for the given session.\n",
    "        z = torch.mm(self.embedding.weight, s_h.T).T\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c0dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [04:06<00:00, 32.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:15<00:00, 66.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [03:11<00:00, 41.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 95.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [03:29<00:00, 38.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:18<00:00, 53.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [03:12<00:00, 41.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 93.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [03:12<00:00, 41.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 94.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [03:11<00:00, 41.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 94.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                         | 4341/8000 [01:43<01:22, 44.23it/s]"
     ]
    }
   ],
   "source": [
    "test_accs, top_k_accs, losses, best_model, best_acc, test_loader = train(args) \n",
    "\n",
    "print(test_accs, top_k_accs)\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\" + \" - \")\n",
    "plt.plot(test_accs, label=\"test accuracy\" + \" - \")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c037806",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test for our best model to save the predictions!\n",
    "test_dataset = GraphDataset('./', 'test')\n",
    "test_loader = pyg_data.DataLoader(test_dataset,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=True)\n",
    "\n",
    "test(test_loader, best_model, is_validation=False, save_model_preds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6425170",
   "metadata": {},
   "source": [
    "# code by jeongiitae6@gmail.com\n",
    "\n",
    "if u have a question , give me the email above address. :)\n",
    "\n",
    "and i special thanks to for allowing the reuse this nice quality resource Eunjee Lynn Sung [at] stanford.edu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

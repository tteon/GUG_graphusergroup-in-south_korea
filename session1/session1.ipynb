{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# message function\n",
        "\n",
        "GCN 구현에 앞서 Graph Neural Network 의 핵심 개념인 Message Passing 을 짚고 넘어가보겠습니다. \n",
        "\n",
        "Message passing 은 총 3가지의 연산으로 이루어진 함수를 의미합니다. 크게 \n",
        "\n",
        "① 정보를 전달하는 __message passing 파트__\n",
        "\n",
        "② 앞서 ①에서의 정보를 바탕으로 hidden state의 갱신을 진행해주는 __update 파트__\n",
        "\n",
        "③ ②에서 갱신된 hidden state의 값을 토대로 output 을 도출해내는 __readout 파트__로\n",
        "로 나누어져있습니다."
      ],
      "metadata": {
        "id": "QSkojE5dK59D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm \n",
        "from scipy.special import softmax\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "HuMfQ5swK6ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message Passing as Matrix Multiplication"
      ],
      "metadata": {
        "id": "Rj-wmQxSR1i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Message Passing notation\n",
        "\n",
        "$$x^{(k)}_i = \\gamma^{(k)}(x^{(k-1)}_i, \\square_{j\\in N(i)} \\phi^{(k)} (x^{(k-1)}_i, x^{(k-1)}_j, e_{j,i})$$\n",
        "\n",
        "\\\n",
        "$$\\square$$ 는 differentiable , permutation invariant 함수를 의미함. 예를들어, 합, 곱, 그리고 평균\n",
        "\n",
        "$$\\gamma \\ and \\ \\phi$$ 는 differentiable function 을 의미함. 예를 들어, MLP "
      ],
      "metadata": {
        "id": "_AVOegBmSRE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array(\n",
        "    [[0,1,0,0,0],\n",
        "     [1,0,1,0,0],\n",
        "     [0,1,0,1,1],\n",
        "     [0,0,1,0,0],\n",
        "     [0,0,1,0,0]]\n",
        ")"
      ],
      "metadata": {
        "id": "iYeERIGWK7GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats = np.arange(A.shape[0]).reshape((-1,1))+1\n",
        "# Adjacency 와 해당 노드의 feature을 곱해줌으로써 update 되는 과정\n",
        "H = A @ feats\n",
        "H"
      ],
      "metadata": {
        "id": "C9SeKzVNK7KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normalization & scale the neighborhood information by neighborhood size"
      ],
      "metadata": {
        "id": "MVCHwG_iNyJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling by neighborhood information size"
      ],
      "metadata": {
        "id": "-uraxvl_Rqp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = np.zeros(A.shape)\n",
        "np.fill_diagonal(D, A.sum(axis=0))\n",
        "print(D , '\\n\\n')\n",
        "\n",
        "D_inv = np.linalg.inv(D)\n",
        "print(D_inv , '\\n\\n')\n",
        "\n",
        "H_avg = D_inv @ A @ feats\n",
        "print(H_avg , '\\n\\n')"
      ],
      "metadata": {
        "id": "7lY8NcS4WzSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalized Adjacency Matrix\n",
        "\n",
        "$$\\hat{A} = \\bar{D}^\\frac{-1}{2} \\bar{A} \\bar{D}^\\frac{-1}{2}$$\n",
        "\n",
        "위 식을 구현하기 위해서 먼저 , $$\\bar{A} = A + I$$ 를 구한다. 이 때 , A는 그래프간의 연결관계를 의미하는 Adjacency matrix 인접행렬 , I 는 Identity matrix 를 의미함.\n",
        "\n",
        "이 때, Identity matrix(항등 행렬)을 포함해주는 이유는 self-connections 를 추가해주는 목적임."
      ],
      "metadata": {
        "id": "opr-dog3ODE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nx.from_numpy_array 는 앞으로 matrix --> network 로 변환하는 용도로 활용도가 높은 함수이니 눈에 익히시는걸 추천드립니다.\n",
        "g = nx.from_numpy_array(A)\n",
        "\n",
        "A_mod = A + np.eye(g.number_of_nodes())"
      ],
      "metadata": {
        "id": "EjkLP9XoO6JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\bar{D}^\\frac{-1}{2}$$ 를 도출하기 위해 다음과 같은 diagonal degree matrix 를 생성해야함.\n",
        "\n",
        "$$(D)_{ij} = \\delta_{i,j}\\sum_kA_{i,k}$$"
      ],
      "metadata": {
        "id": "u9UbWgACZDme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D for A_mod\n",
        "D_mod = np.zeros_like(A_mod)\n",
        "np.fill_diagonal(D_mod, A_mod.sum(axis=1).flatten())\n",
        "\n",
        "# inverse square root of D\n",
        "D_mod_invroot = np.linalg.inv(sqrtm(D_mod))\n",
        "print(D_mod, \"\\n\\n\")\n",
        "\n",
        "print(D_mod_invroot , \"\\n\\n\")"
      ],
      "metadata": {
        "id": "tvhtWCe_PMgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_labels = {i: i+1 for i in range(g.number_of_nodes())}\n",
        "pos = nx.planar_layout(g)"
      ],
      "metadata": {
        "id": "vjHIOBBoZ1Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "nx.draw(\n",
        "    g, pos, with_labels=True,\n",
        "    labels=node_labels,\n",
        "    ax = ax,\n",
        "    edge_color='gray',\n",
        "    node_size=1500,\n",
        "    font_size=30,\n",
        ")"
      ],
      "metadata": {
        "id": "D56NGLk_aACI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\hat{A} = \\bar{D}^\\frac{-1}{2} \\bar{A} \\bar{D}^\\frac{-1}{2} $$\n",
        "\n",
        "에서 $$\\hat{A}$$를 구하기 위해선, 다음과 같은 연산이 필요함.\n",
        "\n",
        "$$\\hat{A}_{i,j} = \\frac{\\bar{A}_{i,j}}{{\\bar{A}_{i,j}} \\sqrt{\\bar{d}_{i}\\bar{d}_{j}}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "kut4lRjgasY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_hat = D_mod_invroot @ A_mod @ D_mod_invroot\n",
        "print(A_hat)"
      ],
      "metadata": {
        "id": "3ekbmAGzcOEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = np.zeros((g.number_of_nodes(), 1))"
      ],
      "metadata": {
        "id": "ZspjWPjzcYJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H[0,0] = 1 # initialize base results\n",
        "H.flatten()"
      ],
      "metadata": {
        "id": "ZszkS7MFcj-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iters = 10\n",
        "results = [H.flatten()]\n",
        "for i in range(iters):\n",
        "  H = H * A_hat\n",
        "  results.append(H.flatten())\n",
        "  print(f\"\\n\\n update iteration {i} results is \\n\\n \" ,results[i])"
      ],
      "metadata": {
        "id": "8iT6RTuFcmb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "NsB7D8Y8c0Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN from scratch\n",
        "\n",
        "[numpy](https://github.com/zjost/blog_code/blob/master/gcn_numpy/gcn_from_scratch.ipynb)\n",
        "\n",
        "[torch](https://theaisummer.com/graph-convolutional-networks/)\n",
        "\n",
        "[GCN paper](https://arxiv.org/abs/1609.02907)\n",
        "\n",
        "[spectral graph](https://angeloyeo.github.io/2019/06/23/Fourier_Series.html) "
      ],
      "metadata": {
        "id": "zOTl3C3tfJcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## numpy"
      ],
      "metadata": {
        "id": "Jeeu7-pCtj8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "from scipy.special import softmax\n",
        "import networkx as nx\n",
        "import networkx.algorithms.community as nx_comm\n",
        "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "%matplotlib inline\n",
        "from IPython.display import HTML\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "118c4lR3fE1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_kkl(nx_G, \n",
        "             label_map,\n",
        "             node_color,\n",
        "             pos=None,\n",
        "             **kwargs):\n",
        "  fig, ax = plt.subplots(figsize=(15,10))\n",
        "  if pos is None:\n",
        "    pos = nx.spring_layout(nx_G, k=5/np.sqrt(nx_G.number_of_nodes()))\n",
        "\n",
        "  nx.draw(\n",
        "      nx_G,\n",
        "      pos,\n",
        "      with_labels=label_map is not None,\n",
        "      labels = label_map,\n",
        "      node_color = node_color,\n",
        "      ax=ax,\n",
        "      **kwargs\n",
        "  )"
      ],
      "metadata": {
        "id": "xFAvdsKTf-nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data download"
      ],
      "metadata": {
        "id": "A0BfEk4Etl1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (dataset)[https://snap.stanford.edu/data/email-Eu-core.html]\n",
        "!wget https://snap.stanford.edu/data/email-Eu-core.txt.gz\n",
        "!gzip -d email-Eu-core.txt.gz"
      ],
      "metadata": {
        "id": "JHSCBK0MoECU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('email-Eu-core.txt',header=None, sep='\\s+', names= ['source','target'])\n",
        "g = nx.from_pandas_edgelist(df)\n",
        "g.number_of_nodes(), g.number_of_edges()"
      ],
      "metadata": {
        "id": "IInL1b1TpZYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-louvain\n",
        "from community import community_louvain\n",
        "partition = community_louvain.best_partition(g)"
      ],
      "metadata": {
        "id": "Qh0nwci0mx3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = np.zeros(g.number_of_nodes())\n",
        "\n",
        "for index, com in enumerate(list(partition.values())):\n",
        "  #print(index,com)\n",
        "  colors[index] = com\n",
        "\n",
        "n_classes = np.unique(colors).shape[0]\n",
        "labels = np.eye(n_classes)[colors.astype(int)]\n",
        "\n",
        "com_labels = nx.get_node_attributes(g,'community')"
      ],
      "metadata": {
        "id": "-A-5yN2Es2y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = draw_kkl(g, None, colors, cmap='gist_rainbow', edge_color='gray')"
      ],
      "metadata": {
        "id": "t-luD10ouSDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Random vs. Power-law Graphs](https://slidetodoc.com/peertopeer-and-social-networks-power-law-graphs-random/)\n"
      ],
      "metadata": {
        "id": "JQD2Dsdb3MPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degree_sequence = sorted((d for n, d in g.degree()), reverse=True)\n",
        "degree_sequencedmax = max(degree_sequence)\n",
        "\n",
        "fig = plt.figure('degree of a email graph', figsize=(15,8))\n",
        "axgrid = fig.add_gridspec(5, 4)\n",
        "\n",
        "ax0 = fig.add_subplot(axgrid[0:3, :])\n",
        "Gcc = g.subgraph(sorted(nx.connected_components(g), key=len, reverse=True)[0])\n",
        "pos = nx.spring_layout(Gcc, seed=20220806)\n",
        "nx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\n",
        "nx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\n",
        "ax0.set_title(\"Connected components of G\")\n",
        "ax0.set_axis_off()\n",
        "\n",
        "ax1 = fig.add_subplot(axgrid[3:, :2])\n",
        "ax1.plot(degree_sequence, \"b-\", marker=\"o\")\n",
        "ax1.set_title(\"Degree Rank Plot\")\n",
        "ax1.set_ylabel(\"Degree\")\n",
        "ax1.set_xlabel(\"Rank\")\n",
        "\n",
        "ax2 = fig.add_subplot(axgrid[3:, 2:])\n",
        "ax2.bar(*np.unique(degree_sequence, return_counts=True))\n",
        "ax2.set_title(\"Degree histogram\")\n",
        "ax2.set_xlabel(\"Degree\")\n",
        "ax2.set_ylabel(\"# of Nodes\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MbRVe3zd1Hk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight related functions"
      ],
      "metadata": {
        "id": "hojjLj9O5HTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [intialize technique](https://yeomko.tistory.com/40)\n",
        "def glorot_init(nin, nout):\n",
        "  sd = np.sqrt(6.0 / (nin + nout))\n",
        "  return np.random.uniform(-sd, sd , size=(nin, nout))\n",
        "\n",
        "def xent(pred, labels):\n",
        "  return -np.log(pred)[np.arange(pred.shape[0]), np.argmax(labels, axis=1)]\n",
        "\n",
        "def norm_diff(dW, dW_approx):\n",
        "  return np.linalg.norm(dW - dW_approx) / (np.linalg.norm(dW) + np.linalg.norm(dW_approx))\n",
        "\n",
        "class GradDescentOptim():\n",
        "  def __init__(self, lr, wd):\n",
        "    self.lr = lr\n",
        "    self.wd = wd\n",
        "    self._y_pred = None\n",
        "    self._y_true = None\n",
        "    self._out = None\n",
        "    self.bs = None\n",
        "    self.train_nodes = None\n",
        "\n",
        "  def __call__(self, y_pred, y_true, train_nodes=None):\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "\n",
        "    if train_nodes is None:\n",
        "      self.train_nodes = np.arange(y_pred.shape[0])\n",
        "    else:\n",
        "      self.train_nodes = train_nodes\n",
        "\n",
        "    self.bs = self.train_nodes.shape[0]\n",
        "\n",
        "  @property\n",
        "  def out(self):\n",
        "    return self._out\n",
        "\n",
        "  @out.setter\n",
        "  def out(self, y):\n",
        "    self._out = y"
      ],
      "metadata": {
        "id": "sMa3J_4X8UTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN Layer "
      ],
      "metadata": {
        "id": "BcNwcW13uG-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer():\n",
        "  def __init__(self, n_inputs, n_outputs, activation=None, name=''):\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_outputs = n_outputs\n",
        "    self.W = glorot_init(self.n_outputs, self.n_inputs)\n",
        "    self.activation = activation\n",
        "    self.name = name\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"GCN: W{'_'+self.name if self.name else ''} ({self.n_inputs}, ({self.n_outputs})\"\n",
        "\n",
        "  def forward(self, A, X, W=None):\n",
        "    '''\n",
        "    A is (bs,bs) adjacency matrix and X is (bs, D)\n",
        "    where bs --> 'batch size' and D --> 'input feature length'\n",
        "    '''\n",
        "    self._A = A\n",
        "    self._X = (A @ X).T # for calculating gradients. \n",
        "\n",
        "    if W is None:\n",
        "      W = self.W\n",
        "    \n",
        "    H = W @ self._X # (h, D) * (D , bs) -> (h, bs)\n",
        "    if self.activation is not None:\n",
        "      H = self.activation(H)\n",
        "    self._H = H # (h, bs)\n",
        "    return self._H.T # (bs, h)\n",
        "\n",
        "  def backward(self, optim, update=True):\n",
        "    dtanh = 1 - np.asarray(self._H.T) ** 2\n",
        "    d2 = np.multiply(optim.out, dtanh) # (bs, out_dim) * element_wise * (bs, out_dim)\n",
        "\n",
        "    self.grad = self._A @ d2 @ self.W\n",
        "    optim.out = self.grad\n",
        "\n",
        "    dW = np.asarray(d2.T @ self._X.T) / optim.bs # (out_dim, bs) * (bs, D) --> (out_dim, D) \n",
        "    dW_wd = self.W * optim.wd / optim.bs # weight decay update\n",
        "\n",
        "    if update:\n",
        "      self.W -= (dW + dW_wd) * optim.lr\n",
        "    \n",
        "    return dW + dW_wd"
      ],
      "metadata": {
        "id": "XOilsykX9jyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation function"
      ],
      "metadata": {
        "id": "FnEqxRu9uK-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxLayer():\n",
        "    def __init__(self, n_inputs, n_outputs, name=''):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.W = glorot_init(self.n_outputs, self.n_inputs)\n",
        "        self.b = np.zeros((self.n_outputs, 1))\n",
        "        self.name = name\n",
        "        self._X = None # Used to calculate gradients\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return f\"Softmax: W{'_'+self.name if self.name else ''} ({self.n_inputs}, {self.n_outputs})\"\n",
        "    \n",
        "    def shift(self, proj):\n",
        "        shiftx = proj - np.max(proj, axis=0, keepdims=True)\n",
        "        exps = np.exp(shiftx)\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "        \n",
        "    def forward(self, X, W=None, b=None):\n",
        "        \"\"\"Compute the softmax of vector x in a numerically stable way.\n",
        "        \n",
        "        X is assumed to be (bs, h)\n",
        "        \"\"\"\n",
        "        self._X = X.T\n",
        "        if W is None:\n",
        "            W = self.W\n",
        "        if b is None:\n",
        "            b = self.b\n",
        "\n",
        "        proj = np.asarray(W @ self._X) + b # (out, h)*(h, bs) = (out, bs)\n",
        "        return self.shift(proj).T # (bs, out)\n",
        "    \n",
        "    def backward(self, optim, update=True):\n",
        "        # should take in optimizer, update its own parameters and update the optimizer's \"out\"\n",
        "        # Build mask on loss\n",
        "        train_mask = np.zeros(optim.y_pred.shape[0])\n",
        "        train_mask[optim.train_nodes] = 1\n",
        "        train_mask = train_mask.reshape((-1, 1))\n",
        "        \n",
        "        # derivative of loss w.r.t. activation (pre-softmax)\n",
        "        d1 = np.asarray((optim.y_pred - optim.y_true)) # (bs, out_dim)\n",
        "        d1 = np.multiply(d1, train_mask) # (bs, out_dim) with loss of non-train nodes set to zero\n",
        "        \n",
        "        self.grad = d1 @ self.W # (bs, out_dim)*(out_dim, in_dim) = (bs, in_dim)\n",
        "        optim.out = self.grad\n",
        "        \n",
        "        dW = (d1.T @ self._X.T) / optim.bs  # (out_dim, bs)*(bs, in_dim) -> (out_dim, in_dim)\n",
        "        db = d1.T.sum(axis=1, keepdims=True) / optim.bs # (out_dim, 1)\n",
        "                \n",
        "        dW_wd = self.W * optim.wd / optim.bs # weight decay update\n",
        "        \n",
        "        if update:   \n",
        "            self.W -= (dW + dW_wd) * optim.lr\n",
        "            self.b -= db.reshape(self.b.shape) * optim.lr\n",
        "        \n",
        "        return dW + dW_wd, db.reshape(self.b.shape)"
      ],
      "metadata": {
        "id": "SeBDT2fg_rEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn1 = GCNLayer(g.number_of_nodes(), 2, activation=np.tanh, name='1')\n",
        "sm1 = SoftmaxLayer(2, n_classes, \"SM\")\n",
        "opt = GradDescentOptim(lr=0, wd=1.)"
      ],
      "metadata": {
        "id": "90tfFunJ_27x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn1_out = gcn1.forward(A_hat, X)\n",
        "opt(sm1.forward(gcn1_out), labels)"
      ],
      "metadata": {
        "id": "dp0XZwcZ_4np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grads(inputs, layer, argname, labels, eps=1e-4, wd=0):\n",
        "    cp = getattr(layer, argname).copy()\n",
        "    cp_flat = np.asarray(cp).flatten()\n",
        "    grads = np.zeros_like(cp_flat)\n",
        "    n_parms = cp_flat.shape[0]\n",
        "    for i, theta in enumerate(cp_flat):\n",
        "        #print(f\"Parm {argname}_{i}\")\n",
        "        theta_cp = theta\n",
        "        \n",
        "        # J(theta + eps)\n",
        "        cp_flat[i] = theta + eps\n",
        "        cp_tmp = cp_flat.reshape(cp.shape)\n",
        "        predp = layer.forward(*inputs, **{argname: cp_tmp})\n",
        "        wd_term = wd/2*(cp_flat**2).sum() / labels.shape[0]\n",
        "        #print(wd_term)\n",
        "        Jp = xent(predp, labels).mean() + wd_term\n",
        "        \n",
        "        # J(theta - eps)\n",
        "        cp_flat[i] = theta - eps\n",
        "        cp_tmp = cp_flat.reshape(cp.shape)\n",
        "        predm = layer.forward(*inputs, **{argname: cp_tmp})\n",
        "        wd_term = wd/2*(cp_flat**2).sum() / labels.shape[0]\n",
        "        #print(wd_term)\n",
        "        Jm = xent(predm, labels).mean() + wd_term\n",
        "        \n",
        "        # grad\n",
        "        grads[i] = ((Jp - Jm) / (2*eps))\n",
        "        \n",
        "        # Back to normal\n",
        "        cp_flat[i] = theta\n",
        "\n",
        "    return grads.reshape(cp.shape)"
      ],
      "metadata": {
        "id": "BCJnnaE6_6Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dW_approx = get_grads((gcn1_out,), sm1, \"W\", labels, eps=1e-4, wd=opt.wd)\n",
        "db_approx = get_grads((gcn1_out,), sm1, \"b\", labels, eps=1e-4, wd=opt.wd)"
      ],
      "metadata": {
        "id": "SefHWM7F_8D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get gradients on Linear Softmax layer\n",
        "dW, db = sm1.backward(opt, update=False)"
      ],
      "metadata": {
        "id": "vYlDY5Ws_9hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert norm_diff(dW, dW_approx) < 1e-7\n",
        "assert norm_diff(db, db_approx) < 1e-7"
      ],
      "metadata": {
        "id": "cqT-AoE2__XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gcn_grads(inputs, gcn, sm_layer, labels, eps=1e-4, wd=0):\n",
        "    cp = gcn.W.copy()\n",
        "    cp_flat = np.asarray(cp).flatten()\n",
        "    grads = np.zeros_like(cp_flat)\n",
        "    n_parms = cp_flat.shape[0]\n",
        "    for i, theta in enumerate(cp_flat):\n",
        "        theta_cp = theta\n",
        "        \n",
        "        # J(theta + eps)\n",
        "        cp_flat[i] = theta + eps\n",
        "        cp_tmp = cp_flat.reshape(cp.shape)\n",
        "        pred = sm_layer.forward(gcn.forward(*inputs, W=cp_tmp))\n",
        "        w2 = (cp_flat**2).sum()+(sm_layer.W.flatten()**2).sum()\n",
        "        Jp = xent(pred, labels).mean() + wd/(2*labels.shape[0])*w2\n",
        "        \n",
        "        # J(theta - eps)\n",
        "        cp_flat[i] = theta - eps\n",
        "        cp_tmp = cp_flat.reshape(cp.shape)\n",
        "        pred = sm_layer.forward(gcn.forward(*inputs, W=cp_tmp))\n",
        "        w2 = (cp_flat**2).sum()+(sm_layer.W.flatten()**2).sum()\n",
        "        Jm = xent(pred, labels).mean() + wd/(2*labels.shape[0])*w2\n",
        "        \n",
        "        # grad\n",
        "        grads[i] = ((Jp - Jm) / (2*eps))\n",
        "        \n",
        "        # Back to normal\n",
        "        cp_flat[i] = theta\n",
        "\n",
        "    return grads.reshape(cp.shape)"
      ],
      "metadata": {
        "id": "-JS9aUoVAAKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dW2 = gcn1.backward(opt, update=False)\n",
        "dW2_approx = get_gcn_grads((A_hat, X), gcn1, sm1, labels, eps=1e-4, wd=opt.wd)\n",
        "assert norm_diff(dW2, dW2_approx) < 1e-7"
      ],
      "metadata": {
        "id": "1euvCwNYABhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gcn_input_grads(A_hat, X, gcn, sm_layer, labels, eps=1e-4):\n",
        "    cp = X.copy()\n",
        "    cp_flat = np.asarray(cp).flatten()\n",
        "    grads = np.zeros_like(cp_flat)\n",
        "    n_parms = cp_flat.shape[0]\n",
        "    for i, x in enumerate(cp_flat):\n",
        "        x_cp = x\n",
        "        \n",
        "        # J(theta + eps)\n",
        "        cp_flat[i] = x + eps\n",
        "        cp_tmp = cp_flat.reshape(cp.shape)\n",
        "        pred = sm_layer.forward(gcn.forward(A_hat, cp_tmp))\n",
        "        Jp = xent(pred, labels).mean()\n",
        "        \n",
        "        # J(theta - eps)\n",
        "        cp_flat[i] = x - eps\n",
        "        cp_tmp = cp_flat.reshape(cp.shape)\n",
        "        pred = sm_layer.forward(gcn.forward(A_hat, cp_tmp))\n",
        "        Jm = xent(pred, labels).mean()\n",
        "        \n",
        "        # grad\n",
        "        grads[i] = ((Jp - Jm) / (2*eps))\n",
        "        \n",
        "        # Back to normal\n",
        "        cp_flat[i] = x\n",
        "\n",
        "    return grads.reshape(cp.shape)"
      ],
      "metadata": {
        "id": "vPZpLZWmAEew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dX_approx = get_gcn_input_grads(A_hat, X, gcn1, sm1, labels, eps=1e-4)\n",
        "assert norm_diff(gcn1.grad/A_hat.shape[0], dX_approx) < 1e-7"
      ],
      "metadata": {
        "id": "8DGbiWPdAJWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNN Model"
      ],
      "metadata": {
        "id": "3M5mp6AZuXMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNNetwork():\n",
        "    def __init__(self, n_inputs, n_outputs, n_layers, hidden_sizes, activation, seed=0):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.activation = activation\n",
        "        \n",
        "        np.random.seed(seed)\n",
        "        \n",
        "        self.layers = list()\n",
        "        # Input layer\n",
        "        gcn_in = GCNLayer(n_inputs, hidden_sizes[0], activation, name='in')\n",
        "        self.layers.append(gcn_in)\n",
        "        \n",
        "        # Hidden layers\n",
        "        for layer in range(n_layers):\n",
        "            gcn = GCNLayer(self.layers[-1].W.shape[0], hidden_sizes[layer], activation, name=f'h{layer}')\n",
        "            self.layers.append(gcn)\n",
        "            \n",
        "        # Output layer\n",
        "        sm_out = SoftmaxLayer(hidden_sizes[-1], n_outputs, name='sm')\n",
        "        self.layers.append(sm_out)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return '\\n'.join([str(l) for l in self.layers])\n",
        "    \n",
        "    def embedding(self, A, X):\n",
        "        # Loop through all GCN layers\n",
        "        H = X\n",
        "        for layer in self.layers[:-1]:\n",
        "            H = layer.forward(A, H)\n",
        "        return np.asarray(H)\n",
        "    \n",
        "    def forward(self, A, X):\n",
        "        # GCN layers\n",
        "        H = self.embedding(A, X)\n",
        "        \n",
        "        # Softmax\n",
        "        p = self.layers[-1].forward(H)\n",
        "        \n",
        "        return np.asarray(p)"
      ],
      "metadata": {
        "id": "RSF2gPuyALiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model = GCNNetwork(\n",
        "    n_inputs=g.number_of_nodes(), \n",
        "    n_outputs=n_classes, \n",
        "    n_layers=2,\n",
        "    hidden_sizes=[16, 2], \n",
        "    activation=np.tanh,\n",
        "    seed=100,\n",
        ")\n",
        "gcn_model"
      ],
      "metadata": {
        "id": "Uu2u0puvBqmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gcn_model.forward(A_hat, X)\n",
        "embed = gcn_model.embedding(A_hat, X)\n",
        "xent(y_pred, labels).mean()"
      ],
      "metadata": {
        "id": "8uv_EmS_Bvgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "obLDBgAuPdsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_nodes = np.array([])\n",
        "test_nodes = np.array([i for i in range(labels.shape[0]) if i not in train_nodes])\n",
        "opt2 = GradDescentOptim(lr=2e-2, wd=2.5e-2)\n",
        "\n",
        "embeds = list()\n",
        "accs = list()\n",
        "train_losses = list()\n",
        "test_losses = list()\n",
        "\n",
        "loss_min = 1e6\n",
        "es_iters = 0\n",
        "es_steps = 50\n",
        "lr_rate_ramp = 0 #-0.05\n",
        "lr_ramp_steps = 1000\n",
        "\n",
        "for epoch in range(15000):\n",
        "    \n",
        "    y_pred = gcn_model.forward(A_hat, X)\n",
        "\n",
        "    opt2(y_pred, labels, train_nodes)\n",
        "    \n",
        "    if ((epoch+1) % lr_ramp_steps) == 0:\n",
        "        opt2.lr *= 1+lr_rate_ramp\n",
        "        print(f\"LR set to {opt2.lr:.4f}\")\n",
        "\n",
        "    for layer in reversed(gcn_model.layers):\n",
        "        layer.backward(opt2, update=True)\n",
        "        \n",
        "    embeds.append(gcn_model.embedding(A_hat, X))\n",
        "    # Accuracy for non-training nodes\n",
        "    acc = (np.argmax(y_pred, axis=1) == np.argmax(labels, axis=1))[\n",
        "        [i for i in range(labels.shape[0]) if i not in train_nodes]\n",
        "    ]\n",
        "    accs.append(acc.mean())\n",
        "    \n",
        "    loss = xent(y_pred, labels)\n",
        "    loss_train = loss[train_nodes].mean()\n",
        "    loss_test = loss[test_nodes].mean()\n",
        "    \n",
        "    train_losses.append(loss_train)\n",
        "    test_losses.append(loss_test)\n",
        "    \n",
        "    if loss_test < loss_min:\n",
        "        loss_min = loss_test\n",
        "        es_iters = 0\n",
        "    else:\n",
        "        es_iters += 1\n",
        "        \n",
        "    if es_iters > es_steps:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch+1}, Train Loss: {loss_train:.3f}, Test Loss: {loss_test:.3f}\")\n",
        "        \n",
        "train_losses = np.array(train_losses)\n",
        "test_losses = np.array(test_losses)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.log10(train_losses), label='Train')\n",
        "ax.plot(np.log10(test_losses), label='Test')\n",
        "ax.legend()\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "Fl4P8tVTBwsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch"
      ],
      "metadata": {
        "id": "MhmocKRpufI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n"
      ],
      "metadata": {
        "id": "gV_5lOGcuhCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data download"
      ],
      "metadata": {
        "id": "7iHVx7-UM8V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (dataset)[https://snap.stanford.edu/data/email-Eu-core.html]\n",
        "!wget https://snap.stanford.edu/data/email-Eu-core.txt.gz\n",
        "!gzip -d email-Eu-core.txt.gz -f\n",
        "\n",
        "# network formatting\n",
        "df = pd.read_csv('email-Eu-core.txt',header=None, sep='\\s+', names= ['source','target'])\n",
        "g = nx.from_pandas_edgelist(df)\n",
        "print(f'node counts ; {g.number_of_nodes()}, edge counts ; {g.number_of_edges()}')\n",
        "\n",
        "# tensor formatting\n",
        "adj_g = torch.Tensor(nx.to_numpy_array(g))\n",
        "print(adj_g.size())"
      ],
      "metadata": {
        "id": "IhidJz6WuxtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### matrix handling"
      ],
      "metadata": {
        "id": "r0yktdOfNAPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_degree_matrix(a):\n",
        "    '''\n",
        "    a ; adjacency matrix\n",
        "    '''\n",
        "    return torch.diag(a.sum(axis=-1))\n",
        "\n",
        "def create_graph_lapl(a):\n",
        "    return calc_degree_matrix(a)-a\n",
        "\n",
        "def calc_degree_matrix_norm(a):\n",
        "    return torch.diag(torch.pow(a.sum(axis=-1),-0.5))\n",
        "\n",
        "def create_graph_lapl_norm(a):\n",
        "    size = a.shape[-1]\n",
        "    D_norm = calc_degree_matrix_norm(a)\n",
        "    L_norm = torch.ones(size) - (D_norm @ a @ D_norm )\n",
        "    return L_norm"
      ],
      "metadata": {
        "id": "N_DFhz_VM_MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chebyshev approximation for the Laplacian powers\n",
        "\n",
        "- Spectral concept -> Laplacian eigenvectors -> SVD of L -> approximation Chebyshev expansion\n",
        "\n",
        "\n",
        "\n",
        "- The Chebyshev approximation is a __recurrent expansion that uses a matrix to estimate the matrix in any given power K__ , thus avoding the K square matrix mulitplications.\n",
        "\n",
        "\n",
        "\n",
        "[한국어자료](https://datalabbit.tistory.com/26)"
      ],
      "metadata": {
        "id": "JRmgwxesNDhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.eig ; https://pytorch.org/docs/stable/generated/torch.eig.html\n",
        "def find_eigmax(L):\n",
        "    '''\n",
        "    L ; Laplaican matrix\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        e1, _ = torch.eig(L, eigenvectors=False)\n",
        "        return torch.max(e1[:, 0]).item()\n",
        "\n",
        "def chebyshev_Lapl(X, Lapl, thetas, order):\n",
        "    list_powers = []\n",
        "    nodes = Lapl.shape[0]\n",
        "\n",
        "    T0 = X.float()\n",
        "\n",
        "    eigmax = find_eigmax(Lapl)\n",
        "    L_rescaled = (2 * Lapl / eigmax) - torch.eye(nodes)\n",
        "\n",
        "    y = T0 * thetas[0]\n",
        "    list_powers.append(y)\n",
        "    T1 = torch.matmul(L_rescaled, T0)\n",
        "    list_powers.append(T1 * thetas[1])\n",
        "\n",
        "    # Computation of: T_k = 2*L_rescaled*T_k-1  -  T_k-2\n",
        "    for k in range(2, order):\n",
        "      T2 = 2 * torch.matmul(L_rescaled, T1) - T0\n",
        "      list_powers.append((T2 * thetas[k]))\n",
        "      T0, T1 = T1, T2\n",
        "    y_out = torch.stack(list_powers, dim=-1)\n",
        "    # the powers may be summed or concatenated. i use concatenation here\n",
        "    y_out = y_out.view(nodes, -1) # -1 = order* features_of_signal\n",
        "    return y_out"
      ],
      "metadata": {
        "id": "ElXMgSRpux5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = 3\n",
        "out_features = 50\n",
        "L = create_graph_lapl_norm(adj_g)\n",
        "x = torch.rand(adj_g.size(-1), features)\n",
        "power_order = 4 # p-hops\n",
        "thetas = nn.Parameter(torch.rand(4))"
      ],
      "metadata": {
        "id": "8SqJBOVHux-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = chebyshev_Lapl(x,L,thetas,power_order)\n",
        "print('cheb approx out powers concatenated:', out.shape)\n",
        "linear = nn.Linear(4*3, out_features)\n",
        "layer_out = linear(out)\n",
        "print(f'Layers output; {layer_out.shape}')"
      ],
      "metadata": {
        "id": "ozFKlRlRuyC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN Layer"
      ],
      "metadata": {
        "id": "6TvEvAEkNP01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchnet"
      ],
      "metadata": {
        "id": "_SbN_eQxOfBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchnet as tnt\n",
        "\n",
        "import os\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "def device_as(x,y):\n",
        "  return x.to(y.device)\n",
        "\n",
        "# tensor operationa now support batched inputs\n",
        "def calc_degree_matrix_norm(a):\n",
        "  return torch.diag_embed(torch.pow(a.sum(dim=-1),-0.5))\n",
        "\n",
        "def create_graph_lapl_norm(a):\n",
        "  size = a.shape[-1]\n",
        "  a +=  device_as(torch.eye(size),a)\n",
        "  D_norm = calc_degree_matrix_norm(a)\n",
        "  L_norm = torch.bmm( torch.bmm(D_norm, a) , D_norm )\n",
        "  return L_norm\n",
        "\n",
        "class GCN_AISUMMER(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "      super().__init__()\n",
        "      self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        \"\"\"\n",
        "        A: adjecency matrix\n",
        "        X: graph signal\n",
        "        \"\"\"\n",
        "        L = create_graph_lapl_norm(A)\n",
        "        x = self.linear(X)\n",
        "        return torch.bmm(L, x)"
      ],
      "metadata": {
        "id": "iuxhf_T8uyJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data download for graph classification task\n",
        "\n",
        "dataset preprcessing 활용되는 라이브러리 \n",
        "\n",
        "[torchnet](https://github.com/pytorch/tnt?fbclid=IwAR0upau00LEWwoBdwhqY5QNT3-L8IUA61mEepkGyAAuKBBEkekfMio-TRNI#getting-started)\n",
        "\n",
        "MUTAH dataset description ; \n",
        "\n",
        "graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels."
      ],
      "metadata": {
        "id": "U0i1wHjLNU6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip\n",
        "!unzip MUTAG.zip\n",
        "!pip install torchnet"
      ],
      "metadata": {
        "id": "-1we-XqPNULg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### graph utils and data loading"
      ],
      "metadata": {
        "id": "fcIHrfdvNvHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indices_to_one_hot(number, nb_classes, label_dummy=-1):\n",
        "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
        "    if number == label_dummy:\n",
        "        return np.zeros(nb_classes)\n",
        "    else:\n",
        "        return np.eye(nb_classes)[number]\n",
        "\n",
        "def get_graph_signal(nx_graph):\n",
        "  d = dict((k, v) for k, v in nx_graph.nodes.items())\n",
        "  x = []\n",
        "  invd = {}\n",
        "  j = 0\n",
        "  for k, v in d.items():\n",
        "      x.append(v['attr_dict'])\n",
        "      invd[k] = j\n",
        "      j = j + 1\n",
        "  return np.array(x)\n",
        "\n",
        "\n",
        "def load_data(path, ds_name, use_node_labels=True, max_node_label=10):\n",
        "    node2graph = {}\n",
        "    Gs = []\n",
        "    data = []\n",
        "    dataset_graph_indicator = f\"{ds_name}_graph_indicator.txt\"\n",
        "    dataset_adj = f\"{ds_name}_A.txt\"\n",
        "    dataset_node_labels = f\"{ds_name}_node_labels.txt\"\n",
        "    dataset_graph_labels = f\"{ds_name}_graph_labels.txt\"\n",
        "\n",
        "    path_graph_indicator = os.path.join(path,dataset_graph_indicator)\n",
        "    path_adj = os.path.join(path,dataset_adj)\n",
        "    path_node_lab = os.path.join(path,dataset_node_labels)\n",
        "    path_labels = os.path.join(path,dataset_graph_labels)\n",
        "\n",
        "\n",
        "    with open(path_graph_indicator, \"r\") as f:\n",
        "        c = 1\n",
        "        for line in f:\n",
        "            node2graph[c] = int(line[:-1])\n",
        "            if not node2graph[c] == len(Gs):\n",
        "                Gs.append(nx.Graph())\n",
        "            Gs[-1].add_node(c)\n",
        "            c += 1\n",
        "\n",
        "    with open(path_adj, \"r\") as f:\n",
        "        for line in f:\n",
        "            edge = line[:-1].split(\",\")\n",
        "            edge[1] = edge[1].replace(\" \", \"\")\n",
        "            Gs[node2graph[int(edge[0])] - 1].add_edge(int(edge[0]), int(edge[1]))\n",
        "\n",
        "    if use_node_labels:\n",
        "      with open(path_node_lab, \"r\") as f:\n",
        "        c = 1\n",
        "        for line in f:\n",
        "          node_label = indices_to_one_hot(int(line[:-1]), max_node_label)\n",
        "          Gs[node2graph[c] - 1].add_node(c, attr_dict=node_label)\n",
        "          c += 1\n",
        "\n",
        "    labels = []\n",
        "    with open(path_labels, \"r\") as f:\n",
        "        for line in f:\n",
        "            labels.append(int(line[:-1]))\n",
        "\n",
        "    return list(zip(Gs, labels)) \n",
        "\n",
        "def create_loaders(dataset, batch_size, split_id, offset=-1):\n",
        "    train_dataset = dataset[:split_id]\n",
        "    val_dataset = dataset[split_id:]\n",
        "    return to_pytorch_dataset(train_dataset, offset,batch_size), to_pytorch_dataset(val_dataset, offset,batch_size)\n",
        "\n",
        "def to_pytorch_dataset(dataset, label_offset=0, batch_size=1):\n",
        "  list_set = []\n",
        "  for graph, label in dataset:\n",
        "    F, G = get_graph_signal(graph), nx.to_numpy_matrix(graph)\n",
        "    numOfNodes = G.shape[0]\n",
        "    F_tensor = torch.from_numpy(F).float()\n",
        "    G_tensor = torch.from_numpy(G).float()\n",
        "\n",
        "    # fix labels to zero-indexing\n",
        "    if label == -1:\n",
        "      label = 0\n",
        "    \n",
        "    label += label_offset\n",
        "    \n",
        "    list_set.append(tuple((F_tensor, G_tensor, label)))\n",
        "\n",
        "  dataset_tnt = tnt.dataset.ListDataset(list_set)\n",
        "  data_loader = torch.utils.data.DataLoader(dataset_tnt, shuffle=True, batch_size=batch_size)\n",
        "  return data_loader\n",
        "\n",
        "\n",
        "\n",
        "dataset = load_data(path='./MUTAG/', ds_name='MUTAG',\n",
        "                  use_node_labels=True, max_node_label=7)\n",
        "train_dataset, val_dataset = create_loaders(dataset, batch_size=1, split_id=150, offset=0)\n",
        "print('Data are ready')"
      ],
      "metadata": {
        "id": "eT_JO5hINot7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNN model "
      ],
      "metadata": {
        "id": "I7S2_c2-N0mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(nn.Module):\n",
        "  def __init__(self,\n",
        "                    in_features = 7,\n",
        "                    hidden_dim = 64,\n",
        "                    classes = 2,\n",
        "                    dropout = 0.5):\n",
        "    super(GNN, self).__init__()\n",
        "\n",
        "    self.conv1 = GCN_AISUMMER(in_features, hidden_dim)\n",
        "    self.conv2 = GCN_AISUMMER(hidden_dim, hidden_dim)\n",
        "    self.conv3 = GCN_AISUMMER(hidden_dim, hidden_dim)\n",
        "    self.fc = nn.Linear(hidden_dim, classes)\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x,A):\n",
        "    x = self.conv1(x, A)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, A)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x, A)\n",
        "    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    # aggregate node embeddings\n",
        "    x = x.mean(dim=1)\n",
        "    # final classification layer\n",
        "    return self.fc(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "W2P1e3IuNy91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "SSNHiBxlN7vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f'Training on {device}')\n",
        "model = GNN(in_features = 7,\n",
        "                hidden_dim = 128,\n",
        "                classes = 2).to(device)\n",
        "\n",
        "optimizer= torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "def train(train_loader):\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader: \n",
        "      optimizer.zero_grad()  \n",
        "      X, A, labels = data\n",
        "      X, A, labels = X.to(device), A.to(device), labels.to(device)  \n",
        "      # Forward pass.\n",
        "      out = model(X, A)  \n",
        "      # Compute the graph classification loss.\n",
        "      loss = criterion(out, labels) \n",
        "      # Calculate gradients.\n",
        "      loss.backward()  \n",
        "      # Updates the models parameters\n",
        "      optimizer.step() \n",
        "\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  for data in loader:\n",
        "    X,A, labels = data\n",
        "    X,A, labels = X.cuda(), A.cuda(), labels.cuda()  \n",
        "    # Forward pass.\n",
        "    out = model(X, A)  \n",
        "    # Take the index of the class with the highest probability.\n",
        "    pred = out.argmax(dim=1) \n",
        "    # Compare with ground-truth labels.\n",
        "    correct += int((pred == labels).sum()) \n",
        "  return correct / len(loader.dataset)  \n",
        "\n",
        "\n",
        "# main code :)\n",
        "\n",
        "best_val = -1\n",
        "for epoch in range(1, 241):\n",
        "    train(train_dataset)\n",
        "    train_acc = test(train_dataset)\n",
        "    val_acc = test(val_dataset)\n",
        "    if val_acc>best_val:\n",
        "      best_val = val_acc\n",
        "      epoch_best = epoch\n",
        "    \n",
        "    if epoch%10==0:\n",
        "      print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} || Best Val Score: {best_val:.4f} (Epoch {epoch_best:03d}) ')"
      ],
      "metadata": {
        "id": "f0kk4Dj0Ny3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Correct & Smooth 로 성능 향상해보기."
      ],
      "metadata": {
        "id": "gfQfIcptfFkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import MLP, CorrectAndSmooth"
      ],
      "metadata": {
        "id": "6QoyNCP-fD9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import MLP, CorrectAndSmooth\n",
        "\n",
        "root = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'OGB')\n",
        "dataset = PygNodePropPredDataset('ogbn-products', root,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "evaluator = Evaluator(name='ogbn-products')\n",
        "split_idx = dataset.get_idx_split()\n",
        "data = dataset[0]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MLP([dataset.num_features, 200, 200, dataset.num_classes], dropout=0.5,\n",
        "            norm=\"batch_norm\", act_first=True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "x, y = data.x.to(device), data.y.to(device)\n",
        "train_idx = split_idx['train'].to(device)\n",
        "val_idx = split_idx['valid'].to(device)\n",
        "test_idx = split_idx['test'].to(device)\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x_train)\n",
        "    loss = criterion(out, y_train.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(out=None):\n",
        "    model.eval()\n",
        "    out = model(x) if out is None else out\n",
        "    pred = out.argmax(dim=-1, keepdim=True)\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y[train_idx],\n",
        "        'y_pred': pred[train_idx]\n",
        "    })['acc']\n",
        "    val_acc = evaluator.eval({\n",
        "        'y_true': y[val_idx],\n",
        "        'y_pred': pred[val_idx]\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y[test_idx],\n",
        "        'y_pred': pred[test_idx]\n",
        "    })['acc']\n",
        "    return train_acc, val_acc, test_acc, out\n",
        "\n",
        "\n",
        "best_val_acc = 0\n",
        "for epoch in range(1, 301):\n",
        "    loss = train()\n",
        "    train_acc, val_acc, test_acc, out = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        y_soft = out.softmax(dim=-1)\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "          f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
        "\n",
        "adj_t = data.adj_t.to(device)\n",
        "deg = adj_t.sum(dim=1).to(torch.float)\n",
        "deg_inv_sqrt = deg.pow_(-0.5)\n",
        "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "DAD = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
        "DA = deg_inv_sqrt.view(-1, 1) * deg_inv_sqrt.view(-1, 1) * adj_t\n",
        "\n",
        "post = CorrectAndSmooth(num_correction_layers=50, correction_alpha=1.0,\n",
        "                        num_smoothing_layers=50, smoothing_alpha=0.8,\n",
        "                        autoscale=False, scale=20.)\n",
        "\n",
        "print('Correct and smooth...')\n",
        "y_soft = post.correct(y_soft, y_train, train_idx, DAD)\n",
        "y_soft = post.smooth(y_soft, y_train, train_idx, DA)\n",
        "print('Done!')\n",
        "train_acc, val_acc, test_acc, _ = test(y_soft)\n",
        "print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "kVoo2euDLacs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Level up\n",
        "\n",
        " 1. Spectral Filtering 관점을 graph transformer 의 positional encoding에 어떻게 적용할 수 있을지 고민해보기\n",
        "\n",
        " 2. [labelSmoothing](https://ratsgo.github.io/insight-notes/docs/interpretable/smoothing) & [C&S](https://seungwooham.tistory.com/entry/CS224w-05-Message-Passing-and-Node-Classification) 관점 이해한 후 , 적용해보기\n",
        "\n",
        " 3. Feature engineering 측면으로 network science 를 어떻게 적용할 수 있을지 고민해보기.\n",
        "\n",
        " 4. Numpy 섹션에서의 community detection 으로 나온 결과와 실제 community label (ground truth) 를 비교해보고 각 community detection 알고리즘 적용 후 왜 그런 결과가 도출됬는지에 대해 고민해보기.\n",
        "\n",
        " 5. over smoothing & over squashing 현상에 대한 이해, 그리고 왜 그런현상이 발생하는지에 대해 graph distribution (그래프 특성)과 연관지어 고민해보기.\n",
        "\n",
        "# Level up 에 관하여 궁금한 점 있으실 시 , 정이태 jeongiitae6@gmail.com 로 연락주세요! :)"
      ],
      "metadata": {
        "id": "zNCHO1YTOrM9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7guUu1px6BON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}